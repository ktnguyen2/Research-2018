{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://embedded-vision.com/platinum-members/bdti/embedded-vision-training/videos/pages/may-2017-embedded-vision-summit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Deep Neural Networks and Convolutional Neural Networks (CNNs)\n",
    "\n",
    "- DNNs learn from the data. Traditional computer approach focuses on hand-engineered classifiers to get the answer out\n",
    "- Classification problem\n",
    "    - Can the computer discern what kind of image? Which kind(s) of objects?\n",
    "    - Challenges:\n",
    "        - Appearance varies by lighting, pose, context, etc\n",
    "        - Clutter\n",
    "        - Fine-grained categorization (horse or exact species)\n",
    "- Shallow vs Deep Learning\n",
    "    - Cascaded classifier = shallow learning\n",
    "- Benefits of DNN\n",
    "    - Same \"toolkit\" to solve many problems:\n",
    "        - Vision -- classification, detection, segmentation, activity recognition\n",
    "        - Speech -- recognition, translation\n",
    "- What is a DNN?\n",
    "    - Neurons = basic computational functional unit (inspired by brain neurons)\n",
    "    - Neuron = weighted sum of inputs, with output fed through some non-linearity\n",
    "- Simplest Deep Net\n",
    "    - 1 input + 1 hidden (layer of neurons) + 1 output layer\n",
    "    - Hidden layer = matrix W (describes input/output transfer function of the layer) and vector b (biases outputs of each neuron)\n",
    "    - During training, an optimizer \"learns\" the best W and b for this problem\n",
    "- Deepening our Network\n",
    "    - Input -> Hidden Layer(s) -> Output\n",
    "    - Multiple neurons grouped together (in parallel) form a layer\n",
    "    - Cascading layers form a network\n",
    "    - DNN = network with more than 2 layers\n",
    "- What Kind of Layers to Stack?\n",
    "    - More layers = more complex transformations\n",
    "    - But linear layers collapse // ultimately end up becoming one layer, so we have to introduce non-linearity\n",
    "- Importance of Non-Linearity\n",
    "    - Rectified Linear Unit (ReLU) -- output = max(0, input) // if input is less than 0, then it becomes 0. if greater than 0, it is left as is\n",
    "    - Place non-linear layer between two linear layers\n",
    "- More Non-Linearities\n",
    "    - Sigmoid\n",
    "    - TanH\n",
    "    - Leakyy ReLU\n",
    "    - ELU\n",
    "    - When in doubt, ReLU\n",
    "    - Worth trying Leaky ReLU and ELU\n",
    "    - Avoid Sigmoid\n",
    "- Designing for Visual Perception\n",
    "    - Visual Structure\n",
    "        - Local processing: pixels close together go together, i.e. receptive fields\n",
    "        - Building up image from simpler features\n",
    "        - Less to more abstract (pixel, motif, part, object)\n",
    "        - Translation invariant across space: looks same no matter where, i.e. filter sharing\n",
    "        - CNNs are a good fit for the visual world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Constituent Layers of CNNs\n",
    "\n",
    "- 3D Convolution\n",
    "    - Input is 3 x 32 x 32 data\n",
    "    - Color image (3 RGB Channels) and square (32 x 32)\n",
    "    - Filter is 3 x 5 x 5 weghts (convolution weights)\n",
    "    - Convolution layers have multiple filters for more modeling capacity\n",
    "- Pooling\n",
    "    - Spatial summary by computing operation over window with stride\n",
    "    - Reduce resolution // suppresses some noise\n",
    "    - Increase receptive field size for later layers\n",
    "    - Save computation\n",
    "    - Add invariance to translation/noise within pooling window\n",
    "- Classifier Layer\n",
    "    - Not actually treated as a layer, but operates on ever element in the last fully-connected layer to generate a probability measure for each of the output classes (ex: horse 70%, dog 50%)\n",
    "    - Softmax = maps real-valued vectors onto a range from 0 to 1 // final probability that the original input lies in a given class\n",
    "- Layer Review\n",
    "    - 3D Convolution: work-horse layer. Tells us what is happening in the scene\n",
    "    - Convolution and Fully-Connected layers have weights and biases whose coefficeints are \"learned\" by the training system\n",
    "    - Spatial resolution reduced by pooling and strides other than 1 in the convolution layers\n",
    "    - Various non-linear activations (e.g. ReLU)\n",
    "- Example DNN Architecture\n",
    "    - Conv 3x3s1, 10 / ReLU\n",
    "        - Conv = Convolver\n",
    "        - s1 = stride 1\n",
    "        - 10 = number of convolvers in the filter bank\n",
    "        - feed linear through non-linearity (ReLU)\n",
    "    - Max Pooling \n",
    "    - FC 10 = unnormalized probability layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Taxonomy of CNNs\n",
    "\n",
    "- LeNet (1989)\n",
    "    - Classify handwritten digits\n",
    "- AlexNet (2012)\n",
    "    - 9 layers\n",
    "- GoogLeNet (2014)\n",
    "    - Inception layer: running layers in parallel and then concatenating them\n",
    "- ResNet (2015)\n",
    "    - Ultra deep, 152 layers\n",
    "    - Hard to train, had to introduce residual net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
