{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ktnng\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'function' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ea0d1af311c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# 28*28 refers to the data set or batch size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m# 28 x 28 weights for 10 output neurons\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'function' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# MNIST Classifier\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 28*28])\n",
    "# 28*28 refers to the data set or batch size\n",
    "\n",
    "w = tf.Variable(tf.zeros([28*28, 10]))\n",
    "# 28 x 28 weights for 10 output neurons\n",
    "\n",
    "b = tf.Variable(tf.zeros([10])\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(x,w)+b)\n",
    "# softmax tell high probability\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y), reduction_indices = [1]))\n",
    "# Cross Entropy\n",
    "# one of many kinds of error types\n",
    "# function to calculate error propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.718281828459045, 7.38905609893065, 20.085536923187668, 54.598150033144236, 2.718281828459045, 7.38905609893065, 20.085536923187668]\n",
      "114.98389973429897\n",
      "[0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]\n"
     ]
    }
   ],
   "source": [
    "# softmax\n",
    "\n",
    "import math\n",
    "\n",
    "z = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]\n",
    "z_exp = [math.exp(i) for i in z]\n",
    "print(z_exp)\n",
    "\n",
    "sum_z_exp = sum(z_exp)\n",
    "print(sum_z_exp)\n",
    "\n",
    "softmax = [round(i / sum_z_exp, 3) for i in z_exp]\n",
    "print(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-14d8c68ee269>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-14d8c68ee269>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    y = tf.matmul(x,w) + b\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Better Approach (has better numerical stability)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 28*28])\n",
    "w = tf.Variable(tf.zeros([28*28, 10]))\n",
    "b = tf.Variable(tf.zeros([10])\n",
    "\n",
    "y = tf.matmul(x,w) + b\n",
    "y_ = tf.placeholder(tf.float32, [None,10])\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "# 0.5 is the learning rate\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "# Convert to floating point\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_preduction, tf.float32))\n",
    "# Calculate the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training data needs to be applied to the model, meaning that weights and biases are updated\n",
    "- MNIST data set needs to be modified for use\n",
    "    - Data is in gzip format files on the internet. It needs to be downloaded and converted\n",
    "    - Labels are an integer. They need to be converted to one-hot\n",
    "- You do not want to train using the entire image set at once\n",
    "    - Data should be broken into mini-batches for training\n",
    "    - Data should be shuffled between training sessions for best results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow MNIST Data Manipulation Class\n",
    "\n",
    "read_data_sets -- class tha thas a number of methods to read data\n",
    "\n",
    "extract_images -- reads gzip file into 4D tensor (dimensions: index, y, x, depth)\n",
    "\n",
    "extract_labels -- reads labels into 1D numpy array\n",
    "\n",
    "dense_to_one_hot -- converts label array to one hot vector array\n",
    "\n",
    "next_batch -- returns the next batch of images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ktnng\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_step = 0 loss = 2.5835933685302734\n",
      "train_step = 100 loss = 0.3879358768463135\n",
      "train_step = 200 loss = 0.2695786654949188\n",
      "train_step = 300 loss = 0.3825521767139435\n",
      "train_step = 400 loss = 0.3337697684764862\n",
      "train_step = 500 loss = 0.23880912363529205\n",
      "train_step = 600 loss = 0.34570667147636414\n",
      "train_step = 700 loss = 0.3182393014431\n",
      "train_step = 800 loss = 0.31698527932167053\n",
      "train_step = 900 loss = 0.15484358370304108\n",
      "train_step = 1000 loss = 0.3292621672153473\n",
      "train_step = 1100 loss = 0.24279601871967316\n",
      "train_step = 1200 loss = 0.24476085603237152\n",
      "train_step = 1300 loss = 0.27919715642929077\n",
      "train_step = 1400 loss = 0.38648104667663574\n",
      "train_step = 1500 loss = 0.34944847226142883\n",
      "train_step = 1600 loss = 0.18796032667160034\n",
      "train_step = 1700 loss = 0.21625633537769318\n",
      "train_step = 1800 loss = 0.32459113001823425\n",
      "train_step = 1900 loss = 0.3251084089279175\n",
      "0.9206\n",
      "0.9206\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Reading the Data Set\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "NUM_PIXELS = 28 * 28\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.5\n",
    "EPOCHS = 2000\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None,NUM_PIXELS])\n",
    "y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES])\n",
    "\n",
    "w = tf.Variable(tf.truncated_normal([NUM_PIXELS,NUM_CLASSES], stddev = 0.1))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "y = tf.matmul(x,w) + b\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "# Training the model\n",
    "train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#Train\n",
    "for t in range(EPOCHS):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    loss,_ = sess.run([cross_entropy, train_step], feed_dict = {x: batch_xs, y_: batch_ys})\n",
    "    if t%100 == 0:\n",
    "        print('train_step = {} loss = {}'.format(t,loss))\n",
    "    \n",
    "# Test the trained model\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(sess.run(accuracy, feed_dict = {x: mnist.test.images, y_: mnist.test.labels}))\n",
    "\n",
    "print(sess.run(accuracy, feed_dict = {x:mnist.test.images, y_:mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
