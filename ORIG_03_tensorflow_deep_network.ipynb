{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this exercise you will explore deeper models, still using the MNIST data set. Again the model follows the lecture closely. \n",
    "\n",
    "This notebook contains code to train a fully connected deep neural network on MNIST. The principal changes from the previous notebook are:\n",
    "\n",
    "* We have switched from a shallow model to a deep neural network.\n",
    "\n",
    "* We are using the AdamOptimizer instead of the vanilla GradientDescentOptimizer.\n",
    "\n",
    "* We are using a much smaller learning rate and running more steps\n",
    "\n",
    "An important takeaway: notice the code to calculate the loss and train the model is identical to the previous notebook, despite the more complex model.\n",
    "\n",
    "Experiment with this notebook by modifying cells and running the cells which contain parameters.\n",
    "\n",
    "Although this is a simple model, we can achieve about >97% accuracy on MNIST, which is impressive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cell 1\n",
    "# notebook version 1.2\n",
    "\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "print ('cell finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell resets the default graph, in case there was a residual from a previous operation, then creates a session for our run. \n",
    "\n",
    "The next few lines specify variables used throughout the rest of the notebook. You can modify the EPOCHS, LEARNING_RATE, and BATCH_SIZE to optimize training time and accuracy, but don't change the number of pixels or classes or your model will fail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cell 2\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "NUM_PIXELS = 28 * 28\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "EPOCHS = 2000\n",
    "LEARNING_RATE = .001\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "print ('cell finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will read the MNIST data into a variable named appropriately, mnist. This will read in the training and testing data such that the training data is mnist.train, and the testing data is mnist.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cell 3\n",
    "\n",
    "mnist = input_data.read_data_sets('/tmp/data', one_hot=True)\n",
    "\n",
    "print ('cell finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will create two placeholders that we will feed with training data initially, and testing data once the model is trained. Once trained for deployment the expected label input will not be necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cell 4\n",
    "\n",
    "# Define input placeholders as in last model. As you will see not much changes around the model\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, NUM_PIXELS])\n",
    "y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES])\n",
    "\n",
    "print ('cell finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell does all the work to create the deep network. Variable `l1w` is the weight tensor for the first hidden layer and variable `l1b` is the biases. The input placeholder `x` is matrix multiplied with the weight tensor, the bias is added and the result passed to a relu activation function returning the hidden layer activation `l1actv`. \n",
    "\n",
    "Activation `l1actv` is the input to the output layer. The output layer is specified by weight `l2w` and bias `l2b`. Again we use a matrix multiplication of the previous layer activation by the layer weights and add the bias. \n",
    "\n",
    "Output `y` is again the inferred classification of the pixel values for the input image. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cell 5\n",
    "\n",
    "# Define the model as in the lecture. There will be a few more variables since we have multiple layers and \n",
    "# intermediate activation outputs. Use tf.nn.relu for the activation functions\n",
    "\n",
    "l1w = tf.Variable(tf.truncated_normal([NUM_PIXELS,500], stddev=0.1))\n",
    "l1b = tf.Variable(tf.constant(0.1,shape=[500]))\n",
    "l1actv = tf.nn.relu(tf.matmul(x,l1w)+ l1b)\n",
    "\n",
    "l2w = tf.Variable(tf.truncated_normal([500,NUM_CLASSES], stddev=0.1))\n",
    "l2b = tf.Variable(tf.constant(0.1,shape=[NUM_CLASSES]))\n",
    "\n",
    "y = tf.matmul(l1actv,l2w) + l2b\n",
    "\n",
    "print ('cell finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines look very similar to the shallow model in the previous exercise but this time we will use the AdamOptimizer, a lower learning rate, and many more training iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cell 6\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_,logits=y))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "\n",
    "print ('cell finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cell 7\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print ('cell finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again try the model before we train it to see what happens. Modify image_index below and try a few values. The range of allowed values is from 0 to 4999. Hit shift-enter to run the cell again and recalculate the value.\n",
    "\n",
    "You will re-run this cell again after the model is trained and see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cell 8\n",
    "\n",
    "image_index = 23\n",
    "exp_label = np.argmax(mnist.test.labels[image_index], 0)\n",
    "x_image = np.reshape(mnist.test.images[image_index], [-1,784])\n",
    "\n",
    "outval = sess.run(y, feed_dict={x:x_image})\n",
    "label= np.argmax(outval[0],0)\n",
    "\n",
    "print (\"calculated label = {} expected label = {}\".format(label, exp_label))\n",
    "pylab.imshow(mnist.test.images[image_index].reshape((28,28)), cmap=pylab.cm.gray_r)   \n",
    "pylab.title('Label: %d' % np.argmax(mnist.test.labels[image_index])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop is exactly the same as the shallow model. Pretty impressive given that we have a much more complex model this time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cell 9\n",
    "\n",
    "for t in range(EPOCHS):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "  loss, _ = sess.run([cross_entropy,train_step], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "  if t%100 == 0:\n",
    "    print('train_step = {} loss = {}'.format(t,loss))\n",
    "\n",
    "print ('cell finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to test the accuracy is also exactly the same as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cell 10\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(sess.run(accuracy, feed_dict={x:mnist.train.images, y_:mnist.train.labels}))\n",
    "\n",
    "print(sess.run(accuracy, feed_dict={x:mnist.test.images, y_:mnist.test.labels}))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with the LEARNING_RATE, EPOCHS, and BATCH_SIZE to see if changing these gets better results. \n",
    "\n",
    "Go back to cell 8 and modify the `image_index` once more to test the trained model. You will see that it behaves very well now that it is trained. \n",
    "\n",
    "Now just for fun add a third layer. Are the results any better?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT: When you are finished make sure you go to the Jupyter notebook “File” menu above and select “Close and halt”. This will shutdown this notebook and take you back to the Jupyter Notebook Home tab.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
